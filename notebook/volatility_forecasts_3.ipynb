{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Volatility Forecasts (Part 2 - XGBoost-STES)\n",
    "\n",
    "This notebook demonstrates the implementation of the Smooth Transition Exponential Smoothing (STES) model. The model is a variant of the Exponential Smoothing (ES) model that captures non-linear dependencies in volatility time series. The STES model is a more advanced version of the ES model that can capture non-linear dependencies in volatility time series. XGBoost-STES is an extension of STES that uses XGBoost to enhance the STES model by better capturing non-linear dependencies in volatility time series.\n",
    "\n",
    "This notebook corresponds to the blog series [Volatility Forecasts (Part 2 - XGBoost-STES)](https://steveya.github.io/2024/07/12/volatility-forecast-2.html). We have refactored the code used [Volatility Forecasts (Part 1 - STES Models)](https://steveya.github.io/blob/notebooks/volatility_forecast_1.ipynb) in The aim is to replace the logistic function used by the STES model with a xgboost model, and evaluate their relative performance. It is a work in progress and will be updated as I wrap up my implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "project_dir = os.path.abspath('..')\n",
    "sys.path.insert(0, project_dir)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from typing import Tuple\n",
    "\n",
    "from volatility_forecast.data.datamanager import (\n",
    "    LagReturnDataManager,\n",
    "    LagAbsReturnDataManager, \n",
    "    LagSquareReturnDataManager,\n",
    "    SquareReturnDataManager,\n",
    ")\n",
    "from volatility_forecast.model.stes_model import STESModel\n",
    "from volatility_forecast.model.neural_network_model import RNNVolatilityModel\n",
    "from volatility_forecast.model.neural_network_model import GRUVolatilityModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Data preparation function\n",
    "def prepare_data(tickers: Tuple[str], start_date: str, end_date: str):\n",
    "    returns = LagReturnDataManager().get_data(tickers, start_date, end_date)[0] * 1e2\n",
    "    realized_var = SquareReturnDataManager().get_data(tickers, start_date, end_date)[0] * 1e4\n",
    "    feature_sets = np.hstack([\n",
    "        np.ones((len(returns), 1)),\n",
    "        LagReturnDataManager().get_data(tickers, start_date, end_date)[0] * 1e2,\n",
    "        LagAbsReturnDataManager().get_data(tickers, start_date, end_date)[0] * 1e2,\n",
    "        LagSquareReturnDataManager().get_data(tickers, start_date, end_date)[0] * 1e4,\n",
    "    ])\n",
    "    return feature_sets, realized_var, returns\n",
    "\n",
    "# Data normalization\n",
    "def normalize_data(X, y):\n",
    "    scaler_X = StandardScaler()\n",
    "    scaler_y = StandardScaler()\n",
    "    X_normalized = scaler_X.fit_transform(X)\n",
    "    y_normalized = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n",
    "    return X_normalized, y_normalized, scaler_X, scaler_y\n",
    "\n",
    "# Model evaluation function\n",
    "def evaluate_model(model, X, y, returns, train_size, test_size):\n",
    "    model.fit(X[:train_size], y[:train_size], returns[:train_size], 0, train_size)\n",
    "    predictions = model.predict(X[train_size:train_size+test_size], returns[train_size:train_size+test_size])\n",
    "    rmse = np.sqrt(mean_squared_error(y[train_size:train_size+test_size], predictions))\n",
    "    return rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "tickers = (\"SPY\",)\n",
    "start_date = \"2000-01-01\"\n",
    "end_date = \"2023-12-31\"\n",
    "\n",
    "X, y, returns = prepare_data(tickers, start_date, end_date)\n",
    "X_normalized, y_normalized, scaler_X, scaler_y = normalize_data(X, y)\n",
    "\n",
    "train_size = 4000\n",
    "test_size = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STES RMSE: 1.661750686869926\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# STES Model\n",
    "stes_model = STESModel()\n",
    "stes_rmse = evaluate_model(stes_model, X, y, returns, train_size, test_size)\n",
    "print(f\"STES RMSE: {stes_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STES RMSE: 2.456403509282011\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# STES Model\n",
    "stes_model = STESModel()\n",
    "stes_rmse_scaled = evaluate_model(stes_model, X_normalized, y_normalized, returns, train_size, test_size)\n",
    "stes_rmse = scaler_y.scale_[0] * stes_rmse_scaled\n",
    "print(f\"STES RMSE: {stes_rmse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/steveyang/miniforge3/envs/py311/lib/python3.11/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([4000, 1])) that is different to the input size (torch.Size([4000])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN RMSE: 1.8421252791229161\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rnn_model = RNNVolatilityModel(input_size=X.shape[1], hidden_size=1)\n",
    "rnn_rmse = evaluate_model(rnn_model, X, y, returns, train_size, test_size)\n",
    "print(f\"RNN RMSE: {rnn_rmse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN RMSE: 3.4246879243225017\n"
     ]
    }
   ],
   "source": [
    "\n",
    "rnn_model = RNNVolatilityModel(input_size=X.shape[1], hidden_size=3)\n",
    "rnn_rmse_scaled = evaluate_model(rnn_model, X_normalized, y_normalized, returns, train_size, test_size)\n",
    "rnn_rmse = scaler_y.scale_[0] * rnn_rmse_scaled\n",
    "print(f\"RNN RMSE: {rnn_rmse}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GRU Model\n",
    "gru_model = GRUVolatilityModel(input_size=X.shape[1], hidden_size=32)\n",
    "gru_rmse = evaluate_model(gru_model, X_normalized, y_normalized, returns, train_size, test_size)\n",
    "gru_rmse = scaler_y.inverse_transform([[gru_rmse]])[0][0]\n",
    "print(f\"GRU RMSE: {gru_rmse}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
